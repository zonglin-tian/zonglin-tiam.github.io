

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid_theme/fluid.png">
  <link rel="icon" href="/img/fluid_theme/conan_b.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Temm">
  <meta name="keywords" content="">
  
    <meta name="description" content="Pytorch study notes">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch Notes">
<meta property="og:url" content="https://zonglin-tian.github.io/2024/04/22/PyTorch-Notes/index.html">
<meta property="og:site_name" content="Temm&#39;s Blog">
<meta property="og:description" content="Pytorch study notes">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-04-22T14:11:34.000Z">
<meta property="article:modified_time" content="2024-04-26T14:40:54.695Z">
<meta property="article:author" content="Temm">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>PyTorch Notes - Temm&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"zonglin-tian.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/fluid_theme/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"TTUkpi4yPOFPvGdrwRvjUPy5-gzGzoHsz","app_key":"ysSAYFHFwtsVmtCrHzIPHVJg","server_url":"https://ttukpi4y.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  <script src="https://zonglin-tian.github.io/live2d/autoload.js"></script>
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Half_Perfect</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/Groceries/" target="_self">
                <i class="iconfont icon-pen"></i>
                <span>杂谈</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/fluid_theme/lm.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="PyTorch Notes"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-04-22 22:11" pubdate>
          2024年4月22日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          1.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          10 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">PyTorch Notes</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    最后更新于: 2024-04-26T22:40:54+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="安装与验证"><a href="#安装与验证" class="headerlink" title="安装与验证"></a>安装与验证</h1><p>通过<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">官网</a>指南安装, 如 CUDA 11.3 + PyTorch 1.12</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 视觉任务通常不需要安装 torchaudio 包</span><br>conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch<br></code></pre></td></tr></table></figure>
<p><strong>验证:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 验证 torch 是否正常安装</span><br>ipython  <span class="hljs-comment"># pip install ipython, ipython 比 python 输入更方便</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-built_in">print</span>(torch.rand(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>))<br><br><span class="hljs-comment"># 验证是否支持 GPU 加速</span><br><span class="hljs-built_in">print</span>(torch.cuda.is_available())<br></code></pre></td></tr></table></figure>


<h1 id="The-Basics"><a href="#The-Basics" class="headerlink" title="The Basics"></a>The Basics</h1><h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p><code>Tensors</code> 是一种特殊的数据结构, 与数组和矩阵非常相似, 在 PyTorch 中, <em>使用 Tensors 表示模型的输入和输出以及模型的参数</em></p>
<ul>
<li>Tensors 与 NumPy 中的 <code>ndarrays</code> 类似, 只是 Tensors 可以在 GPU 或其它硬件加速器上运行</li>
<li>Tensors 可以和 ndarray 共享相同的底层内存, 从而无需复制数据</li>
<li>Tensors 对自动微分进行了优化</li>
</ul>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><ul>
<li><em><strong>shape: 整数元组</strong></em>  –&gt; <code>Tensor.size()</code></li>
<li>dtype</li>
<li>device: 存储设备 (默认 cpu)</li>
<li>dimensions: <code>Tensor.dim()=len(Tensor.shape)</code></li>
<li>total numbel of elments: <code>Tensor.numel()=torch.numel(tensor)</code></li>
</ul>
<h3 id="常见初始化操作"><a href="#常见初始化操作" class="headerlink" title="常见初始化操作"></a>常见初始化操作</h3><ul>
<li>用 <code>tensor</code> 函数转化 python 序列<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]]<br>t_data = torch.tensor(data)<br></code></pre></td></tr></table></figure></li>
<li>用 <code>from_numpy</code> 函数转化 NumPy 数组<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np_array = np.array(data)<br>t_np = torch.from_numpy(np_array)<br></code></pre></td></tr></table></figure></li>
<li>已知 <code>shape</code>, 利用特殊函数创建<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>t_ones = torch.ones(shape)<br>t_zeros = torch.zeros(shape)<br>t_rand = torch.rand(shape, dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># 指定数据类型</span><br></code></pre></td></tr></table></figure></li>
<li>参考已知 Tensors, 利用 <code>*_like</code> 函数创建<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t_ones = torch.ones_like(t_data)<br>t_zeros = torch.zeros_like(t_data)<br>t_rand = torch.rand_like(t_data, dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># 指定数据类型</span><br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="Tensor-创建函数"><a href="#Tensor-创建函数" class="headerlink" title="Tensor 创建函数"></a>Tensor 创建函数</h3><ul>
<li>torch.empty(*size, dtype&#x3D;None): 创建未初始化的 Tensor</li>
<li>torch.zeros(*size, dtype&#x3D;None)</li>
<li>torch.zeros_like(input, dtype&#x3D;None)</li>
<li>torch.ones(*size, dtype&#x3D;None)</li>
<li>torch.randn(*size, dtype&#x3D;None): 随机数据满足标准正太分布</li>
<li>torch.rand(*size, dtype&#x3D;None): 随机数据满足 (0, 1) 上的均匀分布</li>
<li>torh.normal(means, std) 离散正态分布</li>
<li>torch.linspace(start, end, steps): create a one-dimensonal tensor of <code>steps</code> whose value evenly spaced from <code>start</code> to <code>end</code></li>
<li>torch.arange(start&#x3D;0, end, step&#x3D;1, dtype&#x3D;None, device&#x3D;None, requires_grad&#x3D;False): return a 1-D tensor of size <code>(end - start) // step</code> with values from the <code>[start, end)</code> taken with common difference <code>step</code> beginnig from <code>start</code></li>
<li>torch.tensor(data, dtype&#x3D;None, device&#x3D;None, requires_grad&#x3D;False): 用数据构造一个张量</li>
<li>torch.Tensor(data): 用数据构造一个张量</li>
</ul>
<p><strong>Note:</strong></p>
<div class="note note-warning">
            <p>torch.tensor() 是一个函数, 使用时对输入进行拷贝 (不是直接引用), 并根据<em>原始数据</em>生成相应的 torch.LongTensor, torch.FloatTensor, torch.DoubleTensor<br>torch.Tensor() 是一个类 (默认张量类型 torch.FloatTensor() 的别名), 使用时会调用 Tensor 类的构造函数 <strong>init</strong>, 生成<em>单精度浮点类型的张量, 也可仅指定 shape, 此时其可以看作 torch.empty() 的一个特例</em></p>
          </div>

<h3 id="Tensor-的数据类型"><a href="#Tensor-的数据类型" class="headerlink" title="Tensor 的数据类型"></a>Tensor 的数据类型</h3><ul>
<li>torh.FloatTensor() 或 torch.Tenosr(): 32 位浮点数</li>
<li>torch.DoubleTenosr(): 64 位浮点数</li>
<li>torch.ShortTensor(): 16 位整型</li>
<li>torch.IntTensor(): 32 位整型</li>
<li>torch.LongTensor(): 64 位整型</li>
</ul>
<p><strong>数据类型转化:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">tesor = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)  <span class="hljs-comment"># 随机生成 3*5 的 tensor</span><br><span class="hljs-comment"># 转成 long 类型</span><br>newtensor = tensor.long()<br><span class="hljs-comment"># 转成半精度浮点类型 (float16)</span><br>newtensor = tensor.half()<br><span class="hljs-comment"># 转成 int 类型</span><br>newtensor = tensor.<span class="hljs-built_in">int</span>()<br><span class="hljs-comment"># 转成 short 类型</span><br>newtensor = tensor.short()<br><span class="hljs-comment"># 转成 double 类型</span><br>newtensor = tensor.double()<br><span class="hljs-comment"># 转成 float 类型</span><br>newtensor = tensor.<span class="hljs-built_in">float</span>()<br><span class="hljs-comment"># 转成 char 类型</span><br>newtensor = tensor.char()<br><span class="hljs-comment"># 转成 byte 类型</span><br>newtensor = tensor.byte()<br></code></pre></td></tr></table></figure>

<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul>
<li>Tensor.tolist(): return the tensor as a (nested) list. For scalars, a standard Python number is returned like <code>Tensor.item()</code> (only work for tensors with one element)</li>
<li>Tensor.item(): return the value of this tensor as a standard Python number</li>
<li>Tensor.numpy(): 将 Tensor 类型转变为 numpy 类型</li>
<li>Tensor.to(*arg, **kwargs): change an existing tensor’s <code>torch.device</code> and <code>torch.dtype</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>tensor.to(torch.float64)<br>cuda0 = torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<br>tensor.to(cuda0)<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="操作中参数-dim-的理解"><a href="#操作中参数-dim-的理解" class="headerlink" title="操作中参数 dim 的理解"></a>操作中参数 <code>dim</code> 的理解</h2><p><strong>The way to understand the “axis” in NumPy or “dim” in PyTorch is that it <em>collapses the specified axis. So when it collapses</em> the axis 0 (the row), it becomes just one row (column-wise)</strong></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42782150/article/details/106862236">Yale曼陀罗. PyTorch学习笔记——Tensor张量的数据类型的转化、Tensor常见的数据类型、快速创建Tensor. CSDN</a><br>[2] <a target="_blank" rel="noopener" href="https://mathpretty.com/12065.html">王茂南. 理解 PyTorch 中维度的概念</a><br>[3] <a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be">Boyan Barakov. Understanding dimensions in PyTorch</a></p>
<h1 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h1><ul>
<li>torch.mean(input): return the mean value of <strong>all</strong> elements in the <code>input</code> tensor</li>
<li>torch.tensor(data, *, dtype&#x3D;None, device&#x3D;None, requires_grad&#x3D;False): construct a tensor with <em>no</em> autograd history</li>
<li>torch.as_tensor(data, dtype&#x3D;None, device&#x3D;None): convert data into a tensor, sharing data and preserving autograd histroy <em>if possible</em><ul>
<li>torch.from_numpy(ndarray): create a Tensor from a numpy.ndarray, sharing the same memory</li>
</ul>
</li>
<li>torch.log(input): return a new tensor with the natural logirthm of the <em>elements</em> of input, $y_i &#x3D; \ln x_i$<!-- STOP HERE: 2024-04-24/22:32 --></li>
<li>torch.stack(): 沿着一个新维度对输入张量序列进行连接, 序列中所有的张量具有相同形状</li>
<li>torh.flatten(input, start_dim&#x3D;0, end_dim&#x3D;-1): reshape the input into a one-dimensional tensor</li>
<li>torch.split(tensor, split_size_or_sections, dim&#x3D;0): split the tensor into chunks</li>
<li>torch.index_select(input, dim, index, *, out&#x3D;None): <ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_50001789/article/details/120315215">视觉萌新. torch.index_select()——数组索引. CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.index_select.html">TORCH.INDEX_SELECT</a></li>
</ul>
</li>
<li>torch.cross(input, other, dim&#x3D;None): return the cross product of vectors in dimension <code>dim</code> of <code>input</code> and <code>other</code><ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.cross.html">TORCH.CROSS</a></li>
</ul>
</li>
<li>torch.outer(input, vec2): outer product of input and vec2 (可通过坐标生成坐标网格)</li>
<li>torch.inverse(A) 或者 torch.linalg.inv(A): 返回矩阵 A 的逆</li>
<li>torch.matmul(A, B) 或者 A @ B: 矩阵相乘<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/beauthy/article/details/121103704">柏常青. pytorch中的矩阵乘法：函数mul,mm,mv以及 @运算 和 *运算. CSDN</a></li>
</ul>
</li>
</ul>
<h2 id="copy-与-detach"><a href="#copy-与-detach" class="headerlink" title="copy 与 detach"></a>copy 与 detach</h2><p>When we wan to graph some of our tensors. We may have a tensor that requires gradient tracking, but you want a (shallow) copy that does not. This is because <code>matplotlib</code> expects a NumPy array as input, and the implict conversion from a PyTorch tensor to a NumPy array is <strong>not enabled for tensors with <code>requires_grad=True</code></strong>. Making a <code>detached copy</code> lets us move forward.</p>
<h2 id="模型加载与保存"><a href="#模型加载与保存" class="headerlink" title="模型加载与保存"></a>模型加载与保存</h2><ul>
<li>torch.save()</li>
<li>torch.load()<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40522801/article/details/106563354">宁静致远*. Pytorch：模型的保存与加载. CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82038049">鑫鑫淼淼焱焱. PyTorch | 保存和加载模型. 知乎</a></li>
</ul>
</li>
</ul>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">TORCH.OPTIM</a></li>
</ul>
<h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qyhaill/article/details/103043637?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-103043637-blog-109841612.235%5Ev38%5Epc_relevant_sort_base1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-103043637-blog-109841612.235%5Ev38%5Epc_relevant_sort_base1&utm_relevant_index=3">八块腹肌怎么练. torch.optim.lr_scheduler：调整学习率. CSDN</a></li>
</ul>
<h2 id="nn"><a href="#nn" class="headerlink" title="nn"></a>nn</h2><ul>
<li>CrossEntropyLoss<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36201400/article/details/111335423?spm=1001.2101.3001.6650.10&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-10-111335423-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-10-111335423-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&utm_relevant_index=13">仁义礼智信达. torch.nn.CrossEntropyLoss()用法. CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45414792/article/details/120778065?spm=1001.2101.3001.6650.13&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-13-120778065-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-13-120778065-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&utm_relevant_index=16">有点聪明的亚子1. nn.CrossEntropyLoss()交叉熵损失函数. CSDN</a></li>
</ul>
</li>
</ul>
<h2 id="distributed"><a href="#distributed" class="headerlink" title="distributed"></a>distributed</h2><ul>
<li>基本概念<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76638962">Pytorch 分布式训练</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hxxjxw/article/details/119606518">DPP的基本概念</a></li>
</ul>
</li>
<li><code>launch</code>: 分布式训练运行命令<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/magic_ll/article/details/122359490">torch.distributed.launch 命令</a></li>
</ul>
</li>
<li><code>init_process_group()</code>: 分布式训练初始化<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_38252409/article/details/134965424">torch.distributed.init_process_group()详细说明</a></li>
</ul>
</li>
</ul>
<h1 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h1><h2 id="utils"><a href="#utils" class="headerlink" title="utils"></a>utils</h2><ul>
<li>make_grid(tensor, nrow&#x3D;8, padding&#x3D;2, normalize&#x3D;False, range&#x3D;None, sacle_each&#x3D;False, pad_value&#x3D;0): make a grid of images<ul>
<li>tensor [B, C, H, W]</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/generated/torchvision.utils.make_grid.html">torchvision.utils.make_grid</a></li>
</ul>
</li>
<li></li>
</ul>
<h1 id="Auto-Differentiation-with-torch-autograd"><a href="#Auto-Differentiation-with-torch-autograd" class="headerlink" title="Auto Differentiation with torch.autograd"></a>Auto Differentiation with <code>torch.autograd</code></h1><p>By default, all tensors with <code>requires_grad=True</code> are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have thained the model and just want to apply it to some input data, i.e. we only want to <em>forward</em> computations through the network. We can stop tracking computations by surrounding our computiation code with <code>torch.no_grad()</code> block<br>Another way to achieve the same result is to use the <code>detach()</code> method on the tensor<br>There are reasons you might want to disable gradient tracking:</p>
<ul>
<li>To mark some parameters in your neural network as <strong>frozen parameters</strong>.</li>
<li>To <strong>speed up computations</strong> when you are only doing forward pass, because computations on tensors that do not track gardients would be more efficient</li>
</ul>
<p>In a forward pass, autograd does two things simultaneously:</p>
<ul>
<li>run the requested operation to compute a resulting tensor</li>
<li>maintain the operation’s <em>gradient function</em> in the DAG (dicected acyclic graph)</li>
</ul>
<p>The backward pass kicks off when <code>.backward()</code> is called on the DAG root, <code>autograd</code> then:</p>
<ul>
<li>computes the gradients from each <code>.grad_fn</code>,</li>
<li>accumulates them in the respective tensor’s <code>.grad</code> attribute</li>
<li>using the chain rule, propagates all the way to the leaf tensors</li>
</ul>
<p>For a vector functon, a gradient of function to parameters is given by <strong>Jacobian matrix</strong>. Instead of computing the Jacobian matrix itself, PyTorch allows you to compute <strong>Jacobian Product</strong> $v^T\cdot J$ for a given input column tensor $v$. This achieved by calling <code>backward</code> with $v$ as an argument. The size of $v$ should be the same as the size of the vector function.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Coding/" class="category-chain-item">Coding</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Python/" class="print-no-link">#Python</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>PyTorch Notes</div>
      <div>https://zonglin-tian.github.io/2024/04/22/PyTorch-Notes/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Temm</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年4月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/04/27/%E5%86%A5%E6%83%B3%E4%B8%8E%E7%94%9F%E6%B4%BB/" title="冥想与生活">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">冥想与生活</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/19/Ubuntu-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/" title="Ubuntu 文件操作">
                        <span class="hidden-mobile">Ubuntu 文件操作</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'photon-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'zonglin-tian/blog_comments');
      s.setAttribute('issue-term', 'title');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       Lifelong Learner  <a href="https://zonglin-tian.github.io/about" target="_self" rel="nofollow noopener"><span>Temm</span></a> <i class="iconfont icon-love"></i> <a href="https://zonglin-tian.github.io/archives" target="_self" rel="nofollow noopener"><span>Writing</span></a> Running Reading  <a href="https://zonglin-tian.github.io/2024/04/27/冥想与生活" target="_self" rel="nofollow noopener"><span>Meditation</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
